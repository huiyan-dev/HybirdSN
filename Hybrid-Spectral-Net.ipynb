{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# HybridSN 高光谱图像分类\n",
    "\n",
    "S. K. Roy, G. Krishna, S. R. Dubey, B. B. Chaudhuri HybridSN: Exploring 3-D–2-D CNN Feature Hierarchy for Hyperspectral Image Classification, IEEE GRSL 2020\n",
    "\n",
    "这篇论文构建了一个 混合网络 解决高光谱图像分类问题，首先用 3D卷积，然后使用 2D卷积，代码相对简单，下面是代码的解析。\n",
    "\n",
    "数据集已经下载到 src/data  文件夹之中，一共有三类数据{Indian-pines, PaviaU, Salinas}\n",
    "\n",
    "引入基本函数库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
    "import spectral\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. 定义 HybridSN 类\n",
    "\n",
    "模型的网络结构为如下图所示：\n",
    "\n",
    "![HybridSN结构图](./src/img/HybirdSN.png)\n",
    "\n",
    "根据网络结构构建网络如下：\n",
    "\n",
    "三维卷积部分：\n",
    "\n",
    "conv1：（1, 30, 25, 25）， 8个 7x3x3 的卷积核 ==>（8, 24, 23, 23）\n",
    "\n",
    "conv2：（8, 24, 23, 23）， 16个 5x3x3 的卷积核 ==>（16, 20, 21, 21）\n",
    "\n",
    "conv3：（16, 20, 21, 21），32个 3x3x3 的卷积核 ==>（32, 18, 19, 19）\n",
    "\n",
    "接下来要进行二维卷积，因此把前面的 32*18 reshape 一下，得到 （576, 19, 19）\n",
    "\n",
    "二维卷积：（576, 19, 19） 64个 3x3 的卷积核，得到 （64, 17, 17）\n",
    "\n",
    "接下来是一个 flatten 操作，变为 18496 维的向量，\n",
    "\n",
    "接下来依次为256，128节点的全连接层，都使用比例为0.4的 Dropout，\n",
    "\n",
    "最后输出为 16 个节点，是最终的分类类别数。\n",
    "\n",
    "在基础的HybirdSN网络骨架上使用<font color='red'>注意力机制</font>，主要从<font color='red'>空间</font>和<font color='red'>通道</font>两个方面考虑\n",
    "\n",
    "在3D卷积完成后，对输出的特征使用\n",
    "\n",
    "    self.channel_attention_1 = ChannelAttention(576)\n",
    "    self.spatial_attention_1 = SpatialAttention(kernel_size=7)\n",
    "\n",
    "在2D卷积完成后，对输出特征使用\n",
    "\n",
    "    self.channel_attention_1 = ChannelAttention(64)\n",
    "    self.spatial_attention_1 = SpatialAttention(kernel_size=7)\n",
    "\n",
    "下面是 HybridSN 类以及使用注意力机制的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0737, -0.0600,  0.0236,  0.0658,  0.0893, -0.0331,  0.0234,  0.0051,\n",
      "         -0.0023,  0.0463, -0.0039, -0.0243, -0.0757, -0.0388,  0.0743,  0.0404]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 通道注意力\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "# 空间注意力机制\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class HybridSN(nn.Module):\n",
    "    def __init__(self, num_classes, self_attention=False):\n",
    "        super(HybridSN, self).__init__()\n",
    "        # out = (width - kernel_size + 2*padding)/stride + 1\n",
    "        # => padding = ( stride * (out-1) + kernel_size - width)\n",
    "        # 这里因为 stride == 1 所有卷积计算得到的padding都为 0\n",
    "\n",
    "        #默认不使用注意力机制\n",
    "        self.self_attention = self_attention\n",
    "\n",
    "        # 3D卷积块\n",
    "        self.block_1_3D = nn.Sequential(\n",
    "            nn.Conv3d(\n",
    "                in_channels=1,\n",
    "                out_channels=8,\n",
    "                kernel_size=(7, 3, 3),\n",
    "                stride=1,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(\n",
    "                in_channels=8,\n",
    "                out_channels=16,\n",
    "                kernel_size=(5, 3, 3),\n",
    "                stride=1,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=(3, 3, 3),\n",
    "                stride=1,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        if self_attention:\n",
    "            self.channel_attention_1 = ChannelAttention(576)\n",
    "            self.spatial_attention_1 = SpatialAttention(kernel_size=7)\n",
    "        # 2D卷积块\n",
    "        self.block_2_2D = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=576,\n",
    "                out_channels=64,\n",
    "                kernel_size=(3, 3)\n",
    "            ),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        if self_attention:\n",
    "            self.channel_attention_2 = ChannelAttention(64)\n",
    "            self.spatial_attention_2 = SpatialAttention(kernel_size=7)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=18496,\n",
    "                out_features=256\n",
    "            ),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(\n",
    "                in_features=256,\n",
    "                out_features=128\n",
    "            ),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(\n",
    "                in_features=128,\n",
    "                out_features=num_classes\n",
    "            )\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        y = self.block_1_3D(x)\n",
    "        y = y.view(-1, y.shape[1] * y.shape[2], y.shape[3], y.shape[4])\n",
    "        if self.self_attention:\n",
    "            y = self.channel_attention_1(y) * y\n",
    "            y = self.spatial_attention_1(y) * y\n",
    "        y = self.block_2_2D(y)\n",
    "        if self.self_attention:\n",
    "            y = self.channel_attention_2(y) * y\n",
    "            y = self.spatial_attention_2(y) * y\n",
    "        #y = torch.flatten(y.detach())\n",
    "        y = y.view(y.size(0), -1)\n",
    "\n",
    "        y = self.classifier(y)\n",
    "        # y = nn.LogSoftmax(y)\n",
    "        return y\n",
    "        # 全连接层\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 随机输入，测试网络结构是否通\n",
    "    x = torch.randn(1, 1, 30, 25, 25)\n",
    "    net = HybridSN(num_classes=16, self_attention=True)\n",
    "    y = net(x)\n",
    "    #print(y.shape)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. 创建数据集\n",
    "\n",
    "首先对高光谱数据实施PCA降维；然后创建 keras 方便处理的数据格式；然后随机抽取 10% 数据做为训练集，剩余的做为测试集。\n",
    "\n",
    "首先定义基本函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 对高光谱数据 X 应用 PCA 变换\n",
    "def applyPCA(X, numComponents):\n",
    "    newX = np.reshape(X, (-1, X.shape[2]))\n",
    "    pca = PCA(n_components=numComponents, whiten=True)\n",
    "    newX = pca.fit_transform(newX)\n",
    "    newX = np.reshape(newX, (X.shape[0], X.shape[1], numComponents))\n",
    "    return newX\n",
    "\n",
    "# 对单个像素周围提取 patch 时，边缘像素就无法取了，因此，给这部分像素进行 padding 操作\n",
    "def padWithZeros(X, margin=2):\n",
    "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
    "    x_offset = margin\n",
    "    y_offset = margin\n",
    "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
    "    return newX\n",
    "\n",
    "# 在每个像素周围提取 patch ，然后创建成符合 keras 处理的格式\n",
    "def createImageCubes(X, y, windowSize=5, removeZeroLabels = True):\n",
    "    # 给 X 做 padding\n",
    "    margin = int((windowSize - 1) / 2)\n",
    "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
    "    # split patches\n",
    "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n",
    "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n",
    "    patchIndex = 0\n",
    "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
    "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
    "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]\n",
    "            patchesData[patchIndex, :, :, :] = patch\n",
    "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
    "            patchIndex = patchIndex + 1\n",
    "    if removeZeroLabels:\n",
    "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
    "        patchesLabels = patchesLabels[patchesLabels>0]\n",
    "        patchesLabels -= 1\n",
    "    return patchesData, patchesLabels\n",
    "\n",
    "def splitTrainTestSet(X, y, testRatio, randomState=345):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=randomState, stratify=y)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "下面读取并创建数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperspectral data shape:  (145, 145, 200)\n",
      "Label shape:  (145, 145)\n",
      "\n",
      "... ... PCA tranformation ... ...\n",
      "Data shape after PCA:  (145, 145, 30)\n",
      "\n",
      "... ... create data cubes ... ...\n",
      "Data cube X shape:  (10249, 25, 25, 30)\n",
      "Data cube y shape:  (10249,)\n",
      "\n",
      "... ... create train & test data ... ...\n",
      "Xtrain shape:  (1024, 25, 25, 30)\n",
      "Xtest  shape:  (9225, 25, 25, 30)\n",
      "before transpose: Xtrain shape:  (1024, 25, 25, 30, 1)\n",
      "before transpose: Xtest  shape:  (9225, 25, 25, 30, 1)\n",
      "after transpose: Xtrain shape:  (1024, 1, 30, 25, 25)\n",
      "after transpose: Xtest  shape:  (9225, 1, 30, 25, 25)\n"
     ]
    }
   ],
   "source": [
    "# 地物类别\n",
    "class_num = 16\n",
    "X = sio.loadmat('./src/data/Indian_pines_corrected.mat')['indian_pines_corrected']\n",
    "y = sio.loadmat('./src/data/Indian_pines_gt.mat')['indian_pines_gt']\n",
    "\n",
    "# 用于测试样本的比例\n",
    "test_ratio = 0.90\n",
    "# 每个像素周围提取 patch 的尺寸\n",
    "patch_size = 25\n",
    "# 使用 PCA 降维，得到主成分的数量\n",
    "pca_components = 30\n",
    "\n",
    "print('Hyperspectral data shape: ', X.shape)\n",
    "print('Label shape: ', y.shape)\n",
    "\n",
    "print('\\n... ... PCA tranformation ... ...')\n",
    "X_pca = applyPCA(X, numComponents=pca_components)\n",
    "print('Data shape after PCA: ', X_pca.shape)\n",
    "\n",
    "print('\\n... ... create data cubes ... ...')\n",
    "X_pca, y = createImageCubes(X_pca, y, windowSize=patch_size)\n",
    "print('Data cube X shape: ', X_pca.shape)\n",
    "print('Data cube y shape: ', y.shape)\n",
    "\n",
    "print('\\n... ... create train & test data ... ...')\n",
    "Xtrain, Xtest, ytrain, ytest = splitTrainTestSet(X_pca, y, test_ratio)\n",
    "print('Xtrain shape: ', Xtrain.shape)\n",
    "print('Xtest  shape: ', Xtest.shape)\n",
    "\n",
    "# 改变 Xtrain, Ytrain 的形状，以符合 keras 的要求\n",
    "Xtrain = Xtrain.reshape(-1, patch_size, patch_size, pca_components, 1)\n",
    "Xtest  = Xtest.reshape(-1, patch_size, patch_size, pca_components, 1)\n",
    "print('before transpose: Xtrain shape: ', Xtrain.shape)\n",
    "print('before transpose: Xtest  shape: ', Xtest.shape)\n",
    "\n",
    "# 为了适应 pytorch 结构，数据要做 transpose\n",
    "Xtrain = Xtrain.transpose(0, 4, 3, 1, 2)\n",
    "Xtest  = Xtest.transpose(0, 4, 3, 1, 2)\n",
    "print('after transpose: Xtrain shape: ', Xtrain.shape)\n",
    "print('after transpose: Xtest  shape: ', Xtest.shape)\n",
    "\n",
    "\n",
    "\"\"\" Training dataset\"\"\"\n",
    "class TrainDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = Xtrain.shape[0]\n",
    "        self.x_data = torch.FloatTensor(Xtrain)\n",
    "        self.y_data = torch.LongTensor(ytrain)\n",
    "    def __getitem__(self, index):\n",
    "        # 根据索引返回数据和对应的标签\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    def __len__(self):\n",
    "        # 返回文件数据的数目\n",
    "        return self.len\n",
    "\n",
    "\"\"\" Testing dataset\"\"\"\n",
    "class TestDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = Xtest.shape[0]\n",
    "        self.x_data = torch.FloatTensor(Xtest)\n",
    "        self.y_data = torch.LongTensor(ytest)\n",
    "    def __getitem__(self, index):\n",
    "        # 根据索引返回数据和对应的标签\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    def __len__(self):\n",
    "        # 返回文件数据的数目\n",
    "        return self.len\n",
    "\n",
    "# 创建 trainloader 和 testloader\n",
    "trainset = TrainDS()\n",
    "testset  = TestDS()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=128, shuffle=True, num_workers=0)\n",
    "test_loader  = torch.utils.data.DataLoader(dataset=testset,  batch_size=128, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1]   [loss avg: 21.519802]   [current loss: 2.635784]\n",
      "[Epoch: 2]   [loss avg: 20.829661]   [current loss: 2.385289]\n",
      "[Epoch: 3]   [loss avg: 20.266439]   [current loss: 2.535036]\n",
      "[Epoch: 4]   [loss avg: 19.899978]   [current loss: 2.263760]\n",
      "[Epoch: 5]   [loss avg: 19.692733]   [current loss: 2.302357]\n",
      "[Epoch: 6]   [loss avg: 19.472280]   [current loss: 2.326110]\n",
      "[Epoch: 7]   [loss avg: 19.193463]   [current loss: 2.156186]\n",
      "[Epoch: 8]   [loss avg: 18.898808]   [current loss: 2.235258]\n",
      "[Epoch: 9]   [loss avg: 18.566899]   [current loss: 1.896223]\n",
      "[Epoch: 10]   [loss avg: 18.184090]   [current loss: 1.799522]\n",
      "[Epoch: 11]   [loss avg: 17.758273]   [current loss: 1.623863]\n",
      "[Epoch: 12]   [loss avg: 17.307700]   [current loss: 1.481050]\n",
      "[Epoch: 13]   [loss avg: 16.810242]   [current loss: 1.270782]\n",
      "[Epoch: 14]   [loss avg: 16.264767]   [current loss: 1.086986]\n",
      "[Epoch: 15]   [loss avg: 15.668746]   [current loss: 0.744355]\n",
      "[Epoch: 16]   [loss avg: 15.057714]   [current loss: 0.714396]\n",
      "[Epoch: 17]   [loss avg: 14.462856]   [current loss: 0.594860]\n",
      "[Epoch: 18]   [loss avg: 13.872793]   [current loss: 0.412133]\n",
      "[Epoch: 19]   [loss avg: 13.291255]   [current loss: 0.251272]\n",
      "[Epoch: 20]   [loss avg: 12.731942]   [current loss: 0.170961]\n",
      "[Epoch: 21]   [loss avg: 12.200403]   [current loss: 0.269295]\n",
      "[Epoch: 22]   [loss avg: 11.726448]   [current loss: 0.244430]\n",
      "[Epoch: 23]   [loss avg: 11.284742]   [current loss: 0.113467]\n",
      "[Epoch: 24]   [loss avg: 10.853784]   [current loss: 0.088072]\n",
      "[Epoch: 25]   [loss avg: 10.446851]   [current loss: 0.030616]\n",
      "[Epoch: 26]   [loss avg: 10.059302]   [current loss: 0.031802]\n",
      "[Epoch: 27]   [loss avg: 9.694003]   [current loss: 0.010318]\n",
      "[Epoch: 28]   [loss avg: 9.362493]   [current loss: 0.053793]\n",
      "[Epoch: 29]   [loss avg: 9.050510]   [current loss: 0.010090]\n",
      "[Epoch: 30]   [loss avg: 8.761077]   [current loss: 0.039370]\n",
      "[Epoch: 31]   [loss avg: 8.490103]   [current loss: 0.056394]\n",
      "[Epoch: 32]   [loss avg: 8.243266]   [current loss: 0.042738]\n",
      "[Epoch: 33]   [loss avg: 8.008268]   [current loss: 0.036976]\n",
      "[Epoch: 34]   [loss avg: 7.793249]   [current loss: 0.113789]\n",
      "[Epoch: 35]   [loss avg: 7.576945]   [current loss: 0.056090]\n",
      "[Epoch: 36]   [loss avg: 7.372083]   [current loss: 0.013568]\n",
      "[Epoch: 37]   [loss avg: 7.181007]   [current loss: 0.010986]\n",
      "[Epoch: 38]   [loss avg: 6.996236]   [current loss: 0.025332]\n",
      "[Epoch: 39]   [loss avg: 6.818548]   [current loss: 0.002884]\n",
      "[Epoch: 40]   [loss avg: 6.652261]   [current loss: 0.056211]\n",
      "[Epoch: 41]   [loss avg: 6.491622]   [current loss: 0.001425]\n",
      "[Epoch: 42]   [loss avg: 6.339760]   [current loss: 0.001423]\n",
      "[Epoch: 43]   [loss avg: 6.193960]   [current loss: 0.004530]\n",
      "[Epoch: 44]   [loss avg: 6.054249]   [current loss: 0.017398]\n",
      "[Epoch: 45]   [loss avg: 5.921592]   [current loss: 0.004676]\n",
      "[Epoch: 46]   [loss avg: 5.797017]   [current loss: 0.001004]\n",
      "[Epoch: 47]   [loss avg: 5.675653]   [current loss: 0.009086]\n",
      "[Epoch: 48]   [loss avg: 5.560712]   [current loss: 0.005908]\n",
      "[Epoch: 49]   [loss avg: 5.451411]   [current loss: 0.015877]\n",
      "[Epoch: 50]   [loss avg: 5.345706]   [current loss: 0.025430]\n",
      "[Epoch: 51]   [loss avg: 5.242436]   [current loss: 0.004220]\n",
      "[Epoch: 52]   [loss avg: 5.143631]   [current loss: 0.007563]\n",
      "[Epoch: 53]   [loss avg: 5.047675]   [current loss: 0.001253]\n",
      "[Epoch: 54]   [loss avg: 4.954804]   [current loss: 0.001443]\n",
      "[Epoch: 55]   [loss avg: 4.865296]   [current loss: 0.003648]\n",
      "[Epoch: 56]   [loss avg: 4.779958]   [current loss: 0.000774]\n",
      "[Epoch: 57]   [loss avg: 4.696693]   [current loss: 0.000556]\n",
      "[Epoch: 58]   [loss avg: 4.616130]   [current loss: 0.012672]\n",
      "[Epoch: 59]   [loss avg: 4.538012]   [current loss: 0.001753]\n",
      "[Epoch: 60]   [loss avg: 4.462677]   [current loss: 0.001624]\n",
      "[Epoch: 61]   [loss avg: 4.390413]   [current loss: 0.000314]\n",
      "[Epoch: 62]   [loss avg: 4.321091]   [current loss: 0.001873]\n",
      "[Epoch: 63]   [loss avg: 4.252983]   [current loss: 0.001287]\n",
      "[Epoch: 64]   [loss avg: 4.187234]   [current loss: 0.001309]\n",
      "[Epoch: 65]   [loss avg: 4.124772]   [current loss: 0.000626]\n",
      "[Epoch: 66]   [loss avg: 4.063924]   [current loss: 0.008608]\n",
      "[Epoch: 67]   [loss avg: 4.004392]   [current loss: 0.004381]\n",
      "[Epoch: 68]   [loss avg: 3.945816]   [current loss: 0.001852]\n",
      "[Epoch: 69]   [loss avg: 3.889633]   [current loss: 0.002702]\n",
      "[Epoch: 70]   [loss avg: 3.834357]   [current loss: 0.009070]\n",
      "[Epoch: 71]   [loss avg: 3.781339]   [current loss: 0.000299]\n",
      "[Epoch: 72]   [loss avg: 3.729512]   [current loss: 0.017612]\n",
      "[Epoch: 73]   [loss avg: 3.678534]   [current loss: 0.003595]\n",
      "[Epoch: 74]   [loss avg: 3.629113]   [current loss: 0.000115]\n",
      "[Epoch: 75]   [loss avg: 3.581213]   [current loss: 0.000497]\n",
      "[Epoch: 76]   [loss avg: 3.534617]   [current loss: 0.000507]\n",
      "[Epoch: 77]   [loss avg: 3.489991]   [current loss: 0.010618]\n",
      "[Epoch: 78]   [loss avg: 3.445903]   [current loss: 0.018438]\n",
      "[Epoch: 79]   [loss avg: 3.403070]   [current loss: 0.013421]\n",
      "[Epoch: 80]   [loss avg: 3.361604]   [current loss: 0.001828]\n",
      "[Epoch: 81]   [loss avg: 3.320557]   [current loss: 0.000516]\n",
      "[Epoch: 82]   [loss avg: 3.280357]   [current loss: 0.005648]\n",
      "[Epoch: 83]   [loss avg: 3.241197]   [current loss: 0.011361]\n",
      "[Epoch: 84]   [loss avg: 3.202659]   [current loss: 0.000171]\n",
      "[Epoch: 85]   [loss avg: 3.165098]   [current loss: 0.000481]\n",
      "[Epoch: 86]   [loss avg: 3.128416]   [current loss: 0.002168]\n",
      "[Epoch: 87]   [loss avg: 3.092650]   [current loss: 0.000402]\n",
      "[Epoch: 88]   [loss avg: 3.057891]   [current loss: 0.004039]\n",
      "[Epoch: 89]   [loss avg: 3.024102]   [current loss: 0.002690]\n",
      "[Epoch: 90]   [loss avg: 2.990755]   [current loss: 0.004586]\n",
      "[Epoch: 91]   [loss avg: 2.958022]   [current loss: 0.001131]\n",
      "[Epoch: 92]   [loss avg: 2.926058]   [current loss: 0.000285]\n",
      "[Epoch: 93]   [loss avg: 2.895287]   [current loss: 0.018974]\n",
      "[Epoch: 94]   [loss avg: 2.865308]   [current loss: 0.001730]\n",
      "[Epoch: 95]   [loss avg: 2.835622]   [current loss: 0.009104]\n",
      "[Epoch: 96]   [loss avg: 2.806382]   [current loss: 0.001391]\n",
      "[Epoch: 97]   [loss avg: 2.777494]   [current loss: 0.000173]\n",
      "[Epoch: 98]   [loss avg: 2.749207]   [current loss: 0.001360]\n",
      "[Epoch: 99]   [loss avg: 2.721461]   [current loss: 0.000618]\n",
      "[Epoch: 100]   [loss avg: 2.694268]   [current loss: 0.000205]\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# 使用GPU训练，可以在菜单 \"代码执行工具\" -> \"更改运行时类型\" 里进行设置\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 网络放到GPU上\n",
    "net = HybridSN(num_classes=16, self_attention=True).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# 开始训练\n",
    "total_loss = 0\n",
    "net.train() #注意启用训练模式\n",
    "for epoch in range(100):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # 优化器梯度归零\n",
    "        optimizer.zero_grad()\n",
    "        # 正向传播 +　反向传播 + 优化\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print('[Epoch: %d]   [loss avg: %.6f]   [current loss: %.6f]' %(epoch + 1, total_loss/(epoch+1), loss.item()))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     1.0000    0.8293    0.9067        41\n",
      "         1.0     0.9894    0.9463    0.9674      1285\n",
      "         2.0     0.9946    0.9906    0.9926       747\n",
      "         3.0     0.9952    0.9812    0.9882       213\n",
      "         4.0     0.9753    0.9977    0.9864       435\n",
      "         5.0     0.9908    0.9863    0.9886       657\n",
      "         6.0     1.0000    1.0000    1.0000        25\n",
      "         7.0     0.9908    1.0000    0.9954       430\n",
      "         8.0     0.9375    0.8333    0.8824        18\n",
      "         9.0     0.9863    0.9897    0.9880       875\n",
      "        10.0     0.9683    0.9950    0.9815      2210\n",
      "        11.0     0.9720    0.9738    0.9729       534\n",
      "        12.0     1.0000    0.9081    0.9518       185\n",
      "        13.0     0.9913    0.9991    0.9952      1139\n",
      "        14.0     0.9799    0.9827    0.9813       347\n",
      "        15.0     0.8721    0.8929    0.8824        84\n",
      "\n",
      "    accuracy                         0.9819      9225\n",
      "   macro avg     0.9777    0.9566    0.9663      9225\n",
      "weighted avg     0.9821    0.9819    0.9818      9225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "# 模型测试\n",
    "net.eval()  #注意启用测试模式\n",
    "for inputs, _ in test_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = net(inputs)\n",
    "    outputs = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "    if count == 0:\n",
    "        y_pred_test =  outputs\n",
    "        count = 1\n",
    "    else:\n",
    "        y_pred_test = np.concatenate( (y_pred_test, outputs) )\n",
    "\n",
    "# 生成分类报告\n",
    "classification = classification_report(ytest, y_pred_test, digits=4)\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在初始的模型上，如果多跑几次“模型测试代码块”，会发现每次分类的结果都不一样。\n",
    "\n",
    "这是因为原始的代码没有注意到pytorch模型网络的<font color='red'>**训练模式**</font>和<font color='red'>**测试模式**</font>\n",
    "\n",
    "即   <font color='red'>**net.train()**</font> 和 <font color='red'>**net.eval()**</font> (这里net是我们构建的HybirdSN)\n",
    "\n",
    "由于网络的全连接层中使用了 <font color='red'>**nn.Dropout(p=0.4)**</font>\n",
    "\n",
    "训练模式下是启用 Dropout和BN 层的，有些网络层的节点会随机失活。而<font color='red'>**原始代码在测试的时候没有启用测试模式**</font>，即使用的是训练模式，所以每次结果都是不一样的。\n",
    "\n",
    "因此在训练和测试的时候分别启用相应模式就可以保证测试的结果是一样的了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 备用函数\n",
    "下面是用于计算各个类准确率，显示结果的备用函数，以供参考"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from operator import truediv\n",
    "\n",
    "def AA_andEachClassAccuracy(confusion_matrix):\n",
    "    counter = confusion_matrix.shape[0]\n",
    "    list_diag = np.diag(confusion_matrix)\n",
    "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
    "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
    "    average_acc = np.mean(each_acc)\n",
    "    return each_acc, average_acc\n",
    "\n",
    "\n",
    "def reports (test_loader, y_test, name):\n",
    "    count = 0\n",
    "    # 模型测试\n",
    "    for inputs, _ in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = net(inputs)\n",
    "        outputs = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "        if count == 0:\n",
    "            y_pred =  outputs\n",
    "            count = 1\n",
    "        else:\n",
    "            y_pred = np.concatenate( (y_pred, outputs) )\n",
    "\n",
    "    if name == 'IP':\n",
    "        target_names = ['Alfalfa', 'Corn-notill', 'Corn-mintill', 'Corn'\n",
    "                        ,'Grass-pasture', 'Grass-trees', 'Grass-pasture-mowed',\n",
    "                        'Hay-windrowed', 'Oats', 'Soybean-notill', 'Soybean-mintill',\n",
    "                        'Soybean-clean', 'Wheat', 'Woods', 'Buildings-Grass-Trees-Drives',\n",
    "                        'Stone-Steel-Towers']\n",
    "    elif name == 'SA':\n",
    "        target_names = ['Brocoli_green_weeds_1','Brocoli_green_weeds_2','Fallow','Fallow_rough_plow','Fallow_smooth',\n",
    "                        'Stubble','Celery','Grapes_untrained','Soil_vinyard_develop','Corn_senesced_green_weeds',\n",
    "                        'Lettuce_romaine_4wk','Lettuce_romaine_5wk','Lettuce_romaine_6wk','Lettuce_romaine_7wk',\n",
    "                        'Vinyard_untrained','Vinyard_vertical_trellis']\n",
    "    elif name == 'PU':\n",
    "        target_names = ['Asphalt','Meadows','Gravel','Trees', 'Painted metal sheets','Bare Soil','Bitumen',\n",
    "                        'Self-Blocking Bricks','Shadows']\n",
    "\n",
    "    classification = classification_report(y_test, y_pred, target_names=target_names)\n",
    "    oa = accuracy_score(y_test, y_pred)\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "    return classification, confusion, oa*100, each_acc*100, aa*100, kappa*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "检测结果写在文件里："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classification, confusion, oa, each_acc, aa, kappa = reports(test_loader, ytest, 'IP')\n",
    "classification = str(classification)\n",
    "confusion = str(confusion)\n",
    "file_name = \"classification_report.txt\"\n",
    "\n",
    "with open(file_name, 'w') as x_file:\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('{} Kappa accuracy (%)'.format(kappa))\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('{} Overall accuracy (%)'.format(oa))\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('{} Average accuracy (%)'.format(aa))\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('{}'.format(classification))\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('{}'.format(confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "下面代码用于显示分类结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... ... row  0  handling ... ...\n",
      "... ... row  20  handling ... ...\n",
      "... ... row  40  handling ... ...\n",
      "... ... row  60  handling ... ...\n",
      "... ... row  80  handling ... ...\n",
      "... ... row  100  handling ... ...\n",
      "... ... row  120  handling ... ...\n",
      "... ... row  140  handling ... ...\n"
     ]
    }
   ],
   "source": [
    "# load the original image\n",
    "X = sio.loadmat('./src/data/Indian_pines_corrected.mat')['indian_pines_corrected']\n",
    "y = sio.loadmat('./src/data/Indian_pines_gt.mat')['indian_pines_gt']\n",
    "\n",
    "height = y.shape[0]\n",
    "width = y.shape[1]\n",
    "\n",
    "X = applyPCA(X, numComponents= pca_components)\n",
    "X = padWithZeros(X, patch_size//2)\n",
    "\n",
    "# 逐像素预测类别\n",
    "outputs = np.zeros((height,width))\n",
    "for i in range(height):\n",
    "    for j in range(width):\n",
    "        if int(y[i,j]) == 0:\n",
    "            continue\n",
    "        else :\n",
    "            image_patch = X[i:i+patch_size, j:j+patch_size, :]\n",
    "            image_patch = image_patch.reshape(1,image_patch.shape[0],image_patch.shape[1], image_patch.shape[2], 1)\n",
    "            X_test_image = torch.FloatTensor(image_patch.transpose(0, 4, 3, 1, 2)).to(device)\n",
    "            prediction = net(X_test_image)\n",
    "            prediction = np.argmax(prediction.detach().cpu().numpy(), axis=1)\n",
    "            outputs[i][j] = prediction+1\n",
    "    if i % 20 == 0:\n",
    "        print('... ... row ', i, ' handling ... ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\spectral\\graphics\\spypylab.py:27: MatplotlibDeprecationWarning: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "  mpl.rcParams['keymap.all_axes'] = ''\n",
      "D:\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\spectral\\graphics\\spypylab.py:905: MatplotlibDeprecationWarning: Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it.\n",
      "  self.class_axes = plt.imshow(self.class_rgb, **kwargs)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_image = spectral.imshow(classes = outputs.astype(int),figsize =(5,5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}